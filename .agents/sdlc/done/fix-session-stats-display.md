# Fix Session Stats Display (Context Window, Tokens, Costs)

## Context

The session stats display in the SessionNode and SessionStatsDisplay components is broken:

1. **Context window percentage** - Shows 0% when it should show actual usage. The current implementation uses `tokens.input` from the latest assistant message as "current context", but this may be wrong or 0.

2. **Tokens displayed** - Shows 8.3K with a lightning bolt (Zap icon), labeled as "output tokens". This is confusing because:
   - It's unclear what "output" means to users
   - The value may be incorrectly calculated (summing all assistant message outputs)

3. **Costs** - Appears to be correctly calculated (sum of `msg.cost` from all assistant messages)

## Investigation Areas

1. **Data source verification**: Confirm what `AssistantMessage.tokens` contains:
   - `input`: tokens in the prompt sent to the model
   - `output`: tokens generated by the model
   - `reasoning`: thinking/reasoning tokens
   - `cache`: { read, write } for cache hits

2. **Context window calculation**: 
   - Current approach: `latestAssistant.tokens.input` / `DEFAULT_CONTEXT_LIMIT`
   - Problem: If `tokens.input` is 0 or undefined, shows 0%
   - Solution needed: Get actual model context limit from `Model.limit.context`

3. **What stats should we display**:
   - Context window: input tokens / model.limit.context (percentage)
   - Token usage: Could be total input+output, or just output, or breakdown
   - Cost: Sum of all message costs

4. **Model info access**: Need to fetch `Model.limit.context` to get accurate context limits instead of hardcoded `DEFAULT_CONTEXT_LIMIT = 200_000`

## Current Implementation Files

- `components/session/session-stats.tsx` - SessionStatsDisplay component
- `components/graph/session-node.tsx` - SessionNode with inline stats
- `hooks/useSessionStats.ts` - Computation logic
- `context/SyncProvider.tsx` - Data sync via SSE

## Acceptance Criteria

- [ ] Context window percentage accurately reflects tokens.input / model.limit.context
- [ ] Token display is meaningful (consider showing input vs output breakdown)
- [ ] Icons and labels are clear (Gauge for context %, appropriate icons for tokens)
- [ ] Model context limit is fetched from SDK, not hardcoded
- [ ] Both SessionNode and SessionStatsDisplay show consistent, accurate stats

## Notes

2026-01-11: Created work item for exploration. SDK confirms:
- `AssistantMessage.tokens.input` = prompt tokens sent
- `AssistantMessage.tokens.output` = tokens generated
- `Model.limit.context` = max context window size
